{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI8h45bo2C2c"
   },
   "source": [
    "# 第20章 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3yR6qUW2C2d"
   },
   "source": [
    "您可以在Jupyter Notebook查看器(nbviewer.jupyter.org)中查看此笔记本或在Google Colab(colab.research.google.com)中运行它。\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/rickiepark/machine-learning-with-python-cookbook/blob/master/20.ipynb\"><img src=\"https://jupyter.org/assets/share.png\" width=\"60\" />在Jupyter Notebook查看器中查看</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/machine-learning-with-python-cookbook/blob/master/20.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />在Google Colab中执行</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpntUAiQ2C2e"
   },
   "source": [
    "** 注意: 如果您想要使用不包含在TensorFlow的tf.keras API中的多后端Keras，那么请将from tensorflow.keras更改为from keras。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk6OzV3C2C2e"
   },
   "source": [
    "## 20.1 为神经网络预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsL4qcKB2C2e",
    "outputId": "5faffb8f-1820-4665-b297-b04bc0d58938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载库\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# 创建特征\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "\n",
    "# 创建scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# 转换特征\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# 展示特征\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIAH6mar2C2f",
    "outputId": "f119d90c-eccc-43c1-a1d3-f6ff37915e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.0\n",
      "Standard deviation: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# 打印均值和标准差\n",
    "print(\"Mean:\", round(features_standardized[:,0].mean()))\n",
    "print(\"Standard deviation:\", features_standardized[:,0].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr35MEc92C2g"
   },
   "source": [
    "## 20.2 设计一个神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "nMIExGaQ2C2g"
   },
   "outputs": [],
   "source": [
    "# 加载库\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 启动神经网络\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用sigmoid激活函数的全连接层\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 编译神经网络\n",
    "network.compile(loss=\"binary_crossentropy\", # 크로스 엔트로피\n",
    "                optimizer=\"rmsprop\", # 옵티마이저\n",
    "                metrics=[\"accuracy\"]) # 성능 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNr01qcN2C2h"
   },
   "source": [
    "### 打印出模型的概要信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4lZ1Jjo2C2h",
    "outputId": "176e9507-1109-4a56-cea5-9ea616f4bb08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 465\n",
      "Trainable params: 465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "JsPahwMa2C2i"
   },
   "outputs": [],
   "source": [
    "# 连接三个全连接层，从输入到输出\n",
    "x = layers.Input(shape=(10,))\n",
    "h1 = layers.Dense(units=16, activation=\"relu\")(x)\n",
    "h2 = layers.Dense(units=16, activation=\"relu\")(h1)\n",
    "y = layers.Dense(units=1, activation=\"sigmoid\")(h2)\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Model(x, y)\n",
    "\n",
    "# 完成神经网络的模型配置\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDrUauc62C2i",
    "outputId": "1904965a-6d1e-4148-82b2-ccfcaf1858bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 465\n",
      "Trainable params: 465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "IO8H9Qc12C2i"
   },
   "outputs": [],
   "source": [
    "dense = layers.Dense(units=16, activation=\"relu\")\n",
    "h1 = dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SORprZHW2C2j"
   },
   "source": [
    "## 20.3 训练一个二元分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# imdb.npz文件在当前目录下的路径\n",
    "local_imdb_path = os.path.join(current_dir, 'imdb.npz')\n",
    "\n",
    "# 从影评数据中加载数据和目标向量\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    path=local_imdb_path,\n",
    "    num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmFSfjuS2C2j",
    "outputId": "65460a7b-4d69-4790-e3c1-ff56cf9d1dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1s 36us/sample - loss: 0.4198 - accuracy: 0.8127 - val_loss: 0.3377 - val_accuracy: 0.8568\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 29us/sample - loss: 0.3263 - accuracy: 0.8640 - val_loss: 0.3263 - val_accuracy: 0.8613\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 1s 30us/sample - loss: 0.3128 - accuracy: 0.8695 - val_loss: 0.3256 - val_accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "# 加载库\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 设定随机种子\n",
    "np.random.seed(0)\n",
    "\n",
    "# 设定想要的特征数量\n",
    "number_of_features = 1000\n",
    "\n",
    "# 将硬盘数据转化为one-hot编码过的特征向量\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 创建神经网络对象\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "    number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的设置\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 训练神经网络\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=3, # 迭代次数\n",
    "                      verbose=1, # 打印迭代过程\n",
    "                      batch_size=100, # 每个批次的样本数\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkoUmvDR2C2k",
    "outputId": "5b46b6fd-c096-467f-d5bf-962f9307f484"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看特征矩阵的形状\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hdwdw2e2C2k"
   },
   "source": [
    "### 打印出模型的概要信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NHUMPAf2C2k",
    "outputId": "37a795ed-5ffe-401c-ddfd-6b73f499df2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 2s 5ms/step - loss: 0.2987 - val_loss: 0.3241\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2852 - val_loss: 0.3293\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2736 - val_loss: 0.3516\n"
     ]
    }
   ],
   "source": [
    "# 完成神经网络模型的设置\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\") # 优化器\n",
    "\n",
    "# 训练神经网络.\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=3, # 迭代次数\n",
    "                      verbose=1, # 打印迭代过程\n",
    "                      batch_size=100, # 每个批次的样本数\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3Fr2-xP2C2k",
    "outputId": "e66eece7-0baa-4583-cbb0-b00b815f4971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 21us/sample - loss: 0.3261 - accuracy: 0.8613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3261475212478638, 0.86132]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.evaluate(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gutdzvme2C2k"
   },
   "source": [
    "## 20.4 训练一个多元分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PtF4XBC2C2k",
    "outputId": "1d35dd69-7217-4b0c-a21a-40647db083a9"
   },
   "outputs": [],
   "source": [
    "# 加载库\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import reuters\n",
    "# 如果您使用的是多后端Keras，请使用以下代码\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 设定随机种子\n",
    "np.random.seed(0)\n",
    "\n",
    "# 设定我们想要的特征数量\n",
    "number_of_features = 5000\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# reuters.npz文件在当前目录下的路径\n",
    "local_reuters_path = os.path.join(current_dir, 'reuters.npz')\n",
    "\n",
    "# 从路透社新闻数据中加载数据和目标向量\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = reuters.load_data(\n",
    "    path=local_reuters_path,\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# 将特征数据转换为one-hot编码的特征矩阵。\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 将目标向量转换为one-hot编码的目标矩阵。\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Softmax激活函数的全连接层。\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"categorical_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标\n",
    "                      epochs=3, # 3个时代\n",
    "                      verbose=0, # 无输出\n",
    "                      batch_size=100, # 每个批次的样本数\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62AZBuKw2C2l",
    "outputId": "7f50956f-6756-4e5b-a80d-85f8c8823896"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看目标矩阵\n",
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VbZPMAJ2C2l"
   },
   "source": [
    "## 20.5 训练一个回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Dd-wvwTx2C2m"
   },
   "outputs": [],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 创建特征矩阵和目标向量。\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                   n_features = 3,\n",
    "                                   n_informative = 3,\n",
    "                                   n_targets = 1,\n",
    "                                   noise = 0.0,\n",
    "                                   random_state = 0)\n",
    "\n",
    "# 将数据分为训练集和测试集。\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.33, random_state=0)\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=32,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(features_train.shape[1],)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "\n",
    "# 添加一个没有激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"mse\", # 均方误差\n",
    "                optimizer=\"RMSprop\", # 优化器\n",
    "                metrics=[\"mse\"]) # 性能指标\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=10, # 迭代次数\n",
    "                      verbose=0, # 无输出\n",
    "                      batch_size=100, # 每批样本数量\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFumhUMS2C2m"
   },
   "source": [
    "## 20.6 做出预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理内存\n",
    "##### 如果你连续运行了多个内存密集型任务，请确保在运行新任务前清理不再使用的变量并手动调用垃圾收集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 删除变量\n",
    "del data_train, target_train, data_test, target_test\n",
    "gc.collect()\n",
    "\n",
    "del features_train, features_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# imdb.npz文件在当前目录下的路径\n",
    "local_imdb_path = os.path.join(current_dir, 'imdb.npz')\n",
    "\n",
    "# 从影评数据中加载数据和目标向量\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    path=local_imdb_path,\n",
    "    num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "w8z9vQrh2C2m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.3555 - accuracy: 0.8620 - val_loss: 0.2778 - val_accuracy: 0.8890\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 3s 129us/sample - loss: 0.2090 - accuracy: 0.9209 - val_loss: 0.2926 - val_accuracy: 0.8841\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 3s 129us/sample - loss: 0.1703 - accuracy: 0.9364 - val_loss: 0.3330 - val_accuracy: 0.8733\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 指定所需的特征数量。\n",
    "number_of_features = 10000\n",
    "\n",
    "# 将IMDB数据转换为one-hot编码的特征矩阵。\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=3, # 迭代次数\n",
    "                      verbose=1, # 无输出\n",
    "                      batch_size=100, # 每批样本数量\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集的类别。\n",
    "predicted_target = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDoEBHod2C2m",
    "outputId": "683a7c84-ebc5-4aac-c1f3-70f55b0999f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14596137], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查第一个样本被分类为类别1的概率。\n",
    "predicted_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVFvtgtj2C2n"
   },
   "source": [
    "## 20.7 可视化训练历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "V3KQzqHu2C2n"
   },
   "outputs": [],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理内存\n",
    "##### 如果你连续运行了多个内存密集型任务，请确保在运行新任务前清理不再使用的变量并手动调用垃圾收集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 删除变量\n",
    "del data_train, target_train, data_test, target_test\n",
    "gc.collect()\n",
    "\n",
    "del features_train, features_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# imdb.npz文件在当前目录下的路径\n",
    "local_imdb_path = os.path.join(current_dir, 'imdb.npz')\n",
    "\n",
    "# 从影评数据中加载数据和目标向量\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    path=local_imdb_path,\n",
    "    num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "dnpPKITJ2C2n",
    "outputId": "855c6893-9003-4acb-fa3c-d578f2290599"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 랜덤 시드를 설정합니다.\n",
    "np.random.seed(0)\n",
    "\n",
    "# 필요한 특성 개수를 지정합니다.\n",
    "number_of_features = 10000\n",
    "\n",
    "# 영화 리뷰 데이터에서 훈련 데이터와 타깃 벡터를 로드합니다.\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# 영화영화 리뷰 데이터를 원-핫 인코딩된 특성 행렬로 변환합니다.\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 신경망 모델을 만듭니다.\n",
    "network = models.Sequential()\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 시그모이드 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 신경망의 모델 설정을 완료합니다.\n",
    "network.compile(loss=\"binary_crossentropy\", # 크로스 엔트로피\n",
    "                optimizer=\"rmsprop\", # 옵티마이저\n",
    "                metrics=[\"accuracy\"]) # 성능 지표\n",
    "\n",
    "# 신경망을 훈련합니다.\n",
    "history = network.fit(features_train, # 특성\n",
    "                      target_train, # 타깃\n",
    "                      epochs=15, # 에포크 횟수\n",
    "                      verbose=0, # 출력 없음\n",
    "                      batch_size=1000, # 배치의 샘플 개수\n",
    "                      validation_data=(features_test, target_test)) # 테스트 데이터\n",
    "\n",
    "# 훈련 손실과 테스트 손실의 기록을 저장합니다.\n",
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "\n",
    "# 에포크 횟수를 사용해 카운트 객체를 만듭니다.\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# 손실 값의 기록을 시각화합니다.\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "wA1HO-jW2C2n",
    "outputId": "ce3379f9-fb69-497c-8ddc-1bc805a852af",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义epoch_count变量\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# 保存训练精度和测试精度记录。\n",
    "training_accuracy = history.history[\"accuracy\"]\n",
    "test_accuracy = history.history[\"val_accuracy\"]\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "\n",
    "# 绘制精度图。\n",
    "plt.legend([\"训练精度\", \"测试精度\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"精度得分\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1wHjNk62C2n"
   },
   "source": [
    "## 20.8 가중치 규제로 과대적합 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "LvMRILwj2C2n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.6408 - accuracy: 0.8129 - val_loss: 0.4974 - val_accuracy: 0.8555\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 31us/sample - loss: 0.4760 - accuracy: 0.8532 - val_loss: 0.4585 - val_accuracy: 0.8523\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 1s 30us/sample - loss: 0.4416 - accuracy: 0.8560 - val_loss: 0.4266 - val_accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 指定所需的特征数量。\n",
    "number_of_features = 1000\n",
    "\n",
    "# 将电影评论数据转换为one-hot编码的特征矩阵。\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16,\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=3, # 迭代次数\n",
    "                      verbose=1, # 无输出\n",
    "                      batch_size=100, # 每批样本数量\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "C6hr_m1V2C2o"
   },
   "outputs": [],
   "source": [
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer='l1_l2',\n",
    "                         input_shape=(number_of_features,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8boCPmH2C2o"
   },
   "source": [
    "## 20.9 通过提前终止减少过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理内存\n",
    "##### 如果你连续运行了多个内存密集型任务，请确保在运行新任务前清理不再使用的变量并手动调用垃圾收集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 删除变量\n",
    "del data_train, target_train, data_test, target_test\n",
    "gc.collect()\n",
    "\n",
    "del features_train, features_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# imdb.npz文件在当前目录下的路径\n",
    "local_imdb_path = os.path.join(current_dir, 'imdb.npz')\n",
    "\n",
    "# 从影评数据中加载数据和目标向量\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    path=local_imdb_path,\n",
    "    num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "TxxEien42C2o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 1s 39us/sample - loss: 0.4301 - accuracy: 0.8080 - val_loss: 0.3414 - val_accuracy: 0.8556\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 1s 30us/sample - loss: 0.3247 - accuracy: 0.8663 - val_loss: 0.3247 - val_accuracy: 0.8628\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 1s 29us/sample - loss: 0.3120 - accuracy: 0.8705 - val_loss: 0.3255 - val_accuracy: 0.8618\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 1s 29us/sample - loss: 0.3032 - accuracy: 0.8736 - val_loss: 0.3268 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 指定所需的特征数量。\n",
    "number_of_features = 1000\n",
    "\n",
    "# 将电影评论数据转换为one-hot编码的特征矩阵。\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 为了提前终止训练并保存最佳模型，设置回调函数。\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "             ModelCheckpoint(filepath=\"best_model.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True)]\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=20, # 迭代次数\n",
    "                      callbacks=callbacks, # 提前终止\n",
    "                      verbose=1, # 无输出\n",
    "                      batch_size=100, # 每批样本数量\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We_TzWOF2C2o"
   },
   "source": [
    "## 20.10 通过Dropout减少过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "hfqjA9wx2C2o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 랜덤 시드를 설정합니다.\n",
    "np.random.seed(0)\n",
    "\n",
    "# 필요한 특성 개수를 지정합니다.\n",
    "number_of_features = 1000\n",
    "\n",
    "# 영화 리뷰 데이터를 원-핫 인코딩된 특성 행렬로 변환합니다.\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 신경망 모델을 만듭니다.\n",
    "network = models.Sequential()\n",
    "\n",
    "# 입력층으로 드롭아웃 층을 추가합니다.\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "# 시그모이드 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 신경망의 모델 설정을 완료합니다.\n",
    "network.compile(loss=\"binary_crossentropy\", # 크로스 엔트로피\n",
    "                optimizer=\"rmsprop\", # 옵티마이저\n",
    "                metrics=[\"accuracy\"]) # 성능 지표\n",
    "\n",
    "# 신경망을 훈련합니다.\n",
    "history = network.fit(features_train, # 특성\n",
    "                      target_train, # 타깃 벡터\n",
    "                      epochs=3, # 에포크 횟수\n",
    "                      verbose=11, # 출력 없음\n",
    "                      batch_size=100, # 배치의 샘플 개수\n",
    "                      validation_data=(features_test, target_test)) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8tN4gV2C2o"
   },
   "source": [
    "## 20.11 保存模型训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "S8YAdpzJ2C2o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.4460 - accuracy: 0.8035 - val_loss: 0.3425 - val_accuracy: 0.8561\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 29us/sample - loss: 0.3257 - accuracy: 0.8635 - val_loss: 0.3247 - val_accuracy: 0.8625\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 1s 29us/sample - loss: 0.3113 - accuracy: 0.8709 - val_loss: 0.3284 - val_accuracy: 0.8613\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 指定所需的特征数量。\n",
    "number_of_features = 1000\n",
    "\n",
    "# 将电影评论数据转换为one-hot编码的特征矩阵。\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层。\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的设置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 为了提前终止训练并保存最佳模型，设置回调函数。\n",
    "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标向量\n",
    "                      epochs=3, # 迭代次数\n",
    "                      callbacks=checkpoint, # 检查点\n",
    "                      verbose=1, # 无输出\n",
    "                      batch_size=100, # 每批样本数量\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A5Ot0232C2o"
   },
   "source": [
    "## 20.12 使用k折交叉验证评估神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLIbKMBY2C2p",
    "outputId": "60248a14-7beb-4061-8174-88e11d8fa72b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6666 samples\n",
      "Epoch 1/10\n",
      "6666/6666 [==============================] - 0s 39us/sample - loss: 0.8336 - accuracy: 0.5170\n",
      "Epoch 2/10\n",
      "6666/6666 [==============================] - 0s 13us/sample - loss: 0.6864 - accuracy: 0.5638\n",
      "Epoch 3/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.6458 - accuracy: 0.6292\n",
      "Epoch 4/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.6047 - accuracy: 0.6797\n",
      "Epoch 5/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.5614 - accuracy: 0.7184\n",
      "Epoch 6/10\n",
      "6666/6666 [==============================] - 0s 13us/sample - loss: 0.5165 - accuracy: 0.7490\n",
      "Epoch 7/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.4664 - accuracy: 0.7874\n",
      "Epoch 8/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.4070 - accuracy: 0.8293\n",
      "Epoch 9/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.3406 - accuracy: 0.8683\n",
      "Epoch 10/10\n",
      "6666/6666 [==============================] - 0s 12us/sample - loss: 0.2817 - accuracy: 0.9016\n",
      "3334/3334 [==============================] - 0s 19us/sample - loss: 0.3057 - accuracy: 0.8812\n",
      "Train on 6667 samples\n",
      "Epoch 1/10\n",
      "6667/6667 [==============================] - 0s 39us/sample - loss: 0.7000 - accuracy: 0.5281\n",
      "Epoch 2/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.6670 - accuracy: 0.5949\n",
      "Epoch 3/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.6324 - accuracy: 0.6487\n",
      "Epoch 4/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.5819 - accuracy: 0.7123\n",
      "Epoch 5/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.5100 - accuracy: 0.7698\n",
      "Epoch 6/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.4238 - accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.3388 - accuracy: 0.8751\n",
      "Epoch 8/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.2720 - accuracy: 0.9037\n",
      "Epoch 9/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.2264 - accuracy: 0.9202\n",
      "Epoch 10/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.1971 - accuracy: 0.9303\n",
      "3333/3333 [==============================] - 0s 19us/sample - loss: 0.2349 - accuracy: 0.9142\n",
      "Train on 6667 samples\n",
      "Epoch 1/10\n",
      "6667/6667 [==============================] - 0s 39us/sample - loss: 0.7151 - accuracy: 0.5328\n",
      "Epoch 2/10\n",
      "6667/6667 [==============================] - 0s 13us/sample - loss: 0.6595 - accuracy: 0.5985\n",
      "Epoch 3/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.6174 - accuracy: 0.6582\n",
      "Epoch 4/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.5675 - accuracy: 0.7039\n",
      "Epoch 5/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.5088 - accuracy: 0.7548\n",
      "Epoch 6/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.4454 - accuracy: 0.7999\n",
      "Epoch 7/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.3770 - accuracy: 0.8454\n",
      "Epoch 8/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.3096 - accuracy: 0.8841\n",
      "Epoch 9/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.2529 - accuracy: 0.9115\n",
      "Epoch 10/10\n",
      "6667/6667 [==============================] - 0s 12us/sample - loss: 0.2121 - accuracy: 0.9261\n",
      "3333/3333 [==============================] - 0s 19us/sample - loss: 0.2550 - accuracy: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88122374, 0.91419142, 0.90129012])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 特征数量\n",
    "number_of_features = 100\n",
    "\n",
    "# 创建特征矩阵和目标向量。\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.5, .5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "# 创建返回已配置网络的函数。\n",
    "def create_network():\n",
    "\n",
    "    # 创建神经网络模型。\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # 添加使用ReLU激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "        number_of_features,)))\n",
    "\n",
    "    # 再添加一个使用ReLU激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # 添加使用Sigmoid激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # 完成神经网络模型的配置。\n",
    "    network.compile(loss=\"binary_crossentropy\", # 二元交叉熵\n",
    "                    optimizer=\"rmsprop\", # 优化器\n",
    "                    metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "    # 返回配置完成的模型。\n",
    "    return network\n",
    "\n",
    "# 将Keras模型包装起来使其能在scikit-learn中使用。\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "                                 epochs=10,\n",
    "                                 batch_size=100,\n",
    "                                 verbose=1)\n",
    "\n",
    "# 使用3折交叉验证评估神经网络。\n",
    "cross_val_score(neural_network, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R37Bw1D2C2p"
   },
   "source": [
    "## 20.13 调校神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "stV68fON2C2p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Python3.7/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 特征数量\n",
    "number_of_features = 100\n",
    "\n",
    "# 创建特征矩阵和目标向量。\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.5, .5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "# 创建返回已配置网络的函数。\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "\n",
    "    # 创建神经网络模型。\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # 添加使用ReLU激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=16,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(number_of_features,)))\n",
    "\n",
    "    # 再添加一个使用ReLU激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # 添加使用Sigmoid激活函数的全连接层。\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # 完成神经网络模型的配置。\n",
    "    network.compile(loss=\"binary_crossentropy\", # 二元交叉熵\n",
    "                    optimizer=optimizer, # 优化器\n",
    "                    metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "    # 返回配置完成的模型。\n",
    "    return network\n",
    "\n",
    "# 将Keras模型包装起来使其能在scikit-learn中使用。\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
    "\n",
    "# 定义超参数搜索范围。\n",
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "\n",
    "# 创建超参数网格。\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "\n",
    "# 创建网格搜索。\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "\n",
    "# 执行网格搜索。\n",
    "grid_result = grid.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm1KNrSW2C2p",
    "outputId": "14a744fe-d991-4572-8028-854e60c8e74c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10, 'epochs': 5, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查最佳神经网络超参数。\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WtGoE-E2C2p"
   },
   "source": [
    "## 20.14 可视化神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "pN9cSoU62C2p",
    "outputId": "1767ca5f-d204-4586-e113-83d6ee572397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_253 (Dense)            (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 465\n",
      "Trainable params: 465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 创建神经网络模型\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# 再添加一个使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 添加使用Sigmoid激活函数的全连接层\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 使用summary方法打印模型概览\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbpUbtcO2C2q"
   },
   "source": [
    "## 20.15 图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npz文件中的键名: ['x_test', 'x_train', 'y_train', 'y_test']\n",
      "键 'x_test' 对应的数组形状: (10000, 28, 28)\n",
      "键 'x_train' 对应的数组形状: (60000, 28, 28)\n",
      "键 'y_train' 对应的数组形状: (60000,)\n",
      "键 'y_test' 对应的数组形状: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 指定本地 mnist.npz 文件的路径。mnist.npz是一个打包了MNIST数据集的文件，包含训练集和测试集。\n",
    "path = './mnist.npz'\n",
    "\n",
    "# 使用 NumPy 的 load 函数直接加载 .npz 文件。\n",
    "# allow_pickle=True 参数允许加载存储在npz文件中的对象数组，这是因为MNIST数据集的标签是以列表形式存储的。\n",
    "with np.load(path, allow_pickle=True) as f:\n",
    "    # 从文件中提取训练集的图像和标签。\n",
    "    # f['x_train'] 获取训练集图像，这是一个形状为 (60000, 28, 28) 的数组，\n",
    "    # 表示有60,000个样本，每个样本是28x28像素的图像。\n",
    "    data_train, target_train = f['x_train'], f['y_train']\n",
    "    \n",
    "    # 从文件中提取测试集的图像和标签。\n",
    "    # f['x_test'] 获取测试集图像，这是一个形状为 (10000, 28, 28) 的数组，\n",
    "    # 表示有10,000个样本，每个样本是28x28像素的图像。\n",
    "    data_test, target_test = f['x_test'], f['y_test']\n",
    "\n",
    "# 使用NumPy的load函数直接加载.npz文件\n",
    "with np.load(path, allow_pickle=True) as data:\n",
    "    # 打印所有包含的数组的键名\n",
    "    print(\"npz文件中的键名:\", data.files)\n",
    "    \n",
    "    # 遍历所有键名，并打印每个数组的形状\n",
    "    for key in data.files:\n",
    "        print(f\"键 '{key}' 对应的数组形状:\", data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYHqLhig2C2q",
    "outputId": "910fbf0a-09b4-491b-e974-38352edac8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 38s 635us/sample - loss: 0.6133 - accuracy: 0.8098 - val_loss: 0.1737 - val_accuracy: 0.9506\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 38s 633us/sample - loss: 0.2007 - accuracy: 0.9411 - val_loss: 0.0897 - val_accuracy: 0.9712\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 38s 630us/sample - loss: 0.1289 - accuracy: 0.9622 - val_loss: 0.0653 - val_accuracy: 0.9794\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 38s 626us/sample - loss: 0.1010 - accuracy: 0.9697 - val_loss: 0.0503 - val_accuracy: 0.9842\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 38s 634us/sample - loss: 0.0866 - accuracy: 0.9739 - val_loss: 0.0446 - val_accuracy: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fabbd8fd978>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "# 如果使用多后端Keras，请使用以下代码\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# 如果使用多后端Keras，请使用以下代码\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 将颜色通道设置为首个维度。\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 设置图像信息。\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# 将训练图像数据调整为特征的尺寸。\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# 将测试图像数据调整为特征的尺寸。\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "# 将像素强度的范围缩放到0到1之间。\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# 由于改变了数据格式，我们需要重新调整特征的形状\n",
    "# 从原来的(data_train.shape[0], channels, height, width)调整为(data_train.shape[0], height, width, channels)\n",
    "features_train = features_train.reshape(features_train.shape[0], height, width, channels)\n",
    "features_test = features_test.reshape(features_test.shape[0], height, width, channels)\n",
    "\n",
    "# 确保模型的输入层匹配新的数据格式\n",
    "network = Sequential()\n",
    "\n",
    "# 修改输入层的input_shape参数以匹配NHWC格式\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5, 5),\n",
    "                   input_shape=(height, width, channels),\n",
    "                   activation='relu'))\n",
    "\n",
    "# 添加使用2x2窗口的最大池化层。\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 添加一个dropout层。\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# 添加一个展平层以将输入展开为一维向量。\n",
    "network.add(Flatten())\n",
    "\n",
    "# 添加一个128个单元、使用ReLU激活函数的全连接层。\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# 添加一个dropout层。\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# 添加一个使用softmax激活函数的全连接层。\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# 完成神经网络模型的配置。\n",
    "network.compile(loss=\"categorical_crossentropy\", # 交叉熵\n",
    "                optimizer=\"rmsprop\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标\n",
    "\n",
    "# 训练神经网络。\n",
    "network.fit(features_train, # 特征\n",
    "            target_train, # 目标\n",
    "            epochs=5, # 训练轮数\n",
    "            verbose=1, # 显示输出\n",
    "            batch_size=1000, # 批量大小\n",
    "            validation_data=(features_test, target_test)) # 验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qmHmZpk2C2q",
    "outputId": "b94ca3a1-6359-4f9d-a68f-84e4c14287fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 24, 24)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 64, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,182,730\n",
      "Trainable params: 1,182,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.0446 - accuracy: 0.9852\n",
      "Test Loss: 0.04462142323297449\n",
      "Test Accuracy: 0.9851999878883362\n"
     ]
    }
   ],
   "source": [
    "# 测试集上的评估\n",
    "test_loss, test_accuracy = network.evaluate(features_test, target_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 38s 630us/sample - loss: 0.0730 - accuracy: 0.9783 - val_loss: 0.0411 - val_accuracy: 0.9857\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 38s 627us/sample - loss: 0.0661 - accuracy: 0.9796 - val_loss: 0.0389 - val_accuracy: 0.9873\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 38s 627us/sample - loss: 0.0616 - accuracy: 0.9815 - val_loss: 0.0351 - val_accuracy: 0.9883\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 38s 628us/sample - loss: 0.0543 - accuracy: 0.9831 - val_loss: 0.0360 - val_accuracy: 0.9883\n",
      "Epoch 5/5\n",
      "55000/60000 [==========================>...] - ETA: 2s - loss: 0.0522 - accuracy: 0.9843"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练神经网络并保存训练过程中的信息\n",
    "history = network.fit(features_train, target_train, epochs=5, verbose=1, batch_size=1000, validation_data=(features_test, target_test))\n",
    "\n",
    "# 绘制训练过程中的损失\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制训练过程中的准确率\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKAUpgcp2C2q"
   },
   "source": [
    "## 20.16 通过图像增强来改善卷积神经网络的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PClU-UDw2C2q",
    "outputId": "fe6e867f-10bc-4cd5-c9da-eee02c5800fc"
   },
   "outputs": [],
   "source": [
    "# 如果在 Google Colab 上运行，请删除以下注释并执行。\n",
    "import zipfile\n",
    "\n",
    "# 指定要解压缩的文件名\n",
    "file_name = 'raw.zip'\n",
    "\n",
    "# 解压缩文件\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZLz3idj2C2q",
    "outputId": "b001d648-9366-4905-efff-13404552ad55",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12665 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 导入库。\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 创建图像增强对象。\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, # 应用 ZCA 白化。\n",
    "                                  zoom_range=0.3, # 随机缩放图像。\n",
    "                                  width_shift_range=0.2, # 随机移动图像。\n",
    "                                  horizontal_flip=True, # 随机水平翻转图像。\n",
    "                                  rotation_range=90) # 随机旋转图像。\n",
    "\n",
    "# 将增强应用于“raw/images”目录中的所有图像。\n",
    "augment_images = augmentation.flow_from_directory(\"raw/images\", # 图像文件夹\n",
    "                                                  batch_size=32, # 批处理大小\n",
    "                                                  class_mode=\"binary\", # 类别模式\n",
    "                                                  save_to_dir=\"processed/images\") # 将增强后的图像保存到指定目录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPZjma-H2C2q"
   },
   "source": [
    "```python\n",
    "# 训练神经网络模型。\n",
    "network.fit_generator(augment_images,\n",
    "                      # 每个epoch调用生成器的次数。\n",
    "                      steps_per_epoch=2000,\n",
    "                      # epoch数量。\n",
    "                      epochs=5,\n",
    "                      # 验证数据生成器。\n",
    "                      validation_data=augment_images_test,\n",
    "                      # 在每个验证epoch期间调用生成器的次数。\n",
    "                      validation_steps=800)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkzlsJk92C2q",
    "tags": []
   },
   "source": [
    "## 20.17 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "99DMLzSJ2C2q"
   },
   "outputs": [],
   "source": [
    "# 恢复在20.15节中设置的颜色通道位置。\n",
    "# 书中没有此代码。\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MsanVrGI2C2r"
   },
   "outputs": [],
   "source": [
    "# 导入库。\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 设置随机种子。\n",
    "np.random.seed(0)\n",
    "\n",
    "# 指定所需特征的数量。\n",
    "number_of_features = 1000\n",
    "\n",
    "### 从电影评论数据加载训练数据和目标向量。\n",
    "import os\n",
    "\n",
    "# 当前文件的绝对路径\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# imdb.npz文件在当前目录下的路径\n",
    "local_imdb_path = os.path.join(current_dir, 'imdb.npz')\n",
    "\n",
    "# 从影评数据中加载数据和目标向量\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    path=local_imdb_path,\n",
    "    num_words=number_of_features)\n",
    "\n",
    "###\n",
    "\n",
    "# 对每个样本进行填充或截断，使其具有400个特征。\n",
    "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "\n",
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加嵌入层。\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "\n",
    "# 添加具有128个单元的LSTM层。\n",
    "network.add(layers.LSTM(units=128))\n",
    "\n",
    "# 添加具有sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的配置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 二元交叉熵\n",
    "                optimizer=\"Adam\", # 优化器\n",
    "                metrics=[\"accuracy\"]) # 性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 259,713\n",
      "Trainable params: 259,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0326 22:16:29.641951 140174145750848 deprecation.py:323] From /Python3.7/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-db54f430b51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 不显示输出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 批量大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                       validation_data=(features_test, target_test)) # 测试数据\n\u001b[0m",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标\n",
    "                      epochs=3, # 训练轮数\n",
    "                      verbose=1, # 不显示输出\n",
    "                      batch_size=1000, # 批量大小\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9E4QMQO2C2r",
    "outputId": "69619541-9443-4be2-f813-1cb06056afe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# 查看第一个样本\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTP4joHG2C2r",
    "outputId": "a7a26254-4622-42f8-a834-11b4f55d820a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1 591 202  14  31   6 717  10  10   2\n",
      "   2   5   4 360   7   4 177   2 394 354   4 123   9   2   2   2  10  10\n",
      "  13  92 124  89 488   2 100  28   2  14  31  23  27   2  29 220 468   8\n",
      " 124  14 286 170   8 157  46   5  27 239  16 179   2  38  32  25   2 451\n",
      " 202  14   6 717]\n"
     ]
    }
   ],
   "source": [
    "# 查看第一个样本\n",
    "print(features_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x-5XNZ_2C2r"
   },
   "source": [
    "### 附录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "-uDwUWOt2C2r"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1000,400,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node embedding_3/embedding_lookup (defined at <ipython-input-162-d1dd546c27f1>:24) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_1835327]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-d1dd546c27f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 不显示输出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 批量大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                       validation_data=(features_test, target_test)) # 测试数据\n\u001b[0m",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Python3.7/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1000,400,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node embedding_3/embedding_lookup (defined at <ipython-input-162-d1dd546c27f1>:24) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_1835327]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# 创建神经网络模型。\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加嵌入层。\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "\n",
    "# 添加包含128个单元的GRU层。\n",
    "network.add(layers.GRU(units=128))\n",
    "\n",
    "# 添加使用sigmoid激活函数的全连接层。\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 完成神经网络模型的配置。\n",
    "network.compile(loss=\"binary_crossentropy\", # 二元交叉熵\n",
    "                optimizer=\"Adam\", # Adam优化器\n",
    "                metrics=[\"accuracy\"]) # 准确率指标\n",
    "\n",
    "# 训练神经网络。\n",
    "history = network.fit(features_train, # 特征\n",
    "                      target_train, # 目标\n",
    "                      epochs=3, # 训练轮数\n",
    "                      verbose=0, # 不显示输出\n",
    "                      batch_size=1000, # 批量大小\n",
    "                      validation_data=(features_test, target_test)) # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
